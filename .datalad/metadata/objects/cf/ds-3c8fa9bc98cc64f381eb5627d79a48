{
"@context":
{
"@vocab":
"http://docs.datalad.org/schema_v2.0.json"
},
"bids":
{
"@context":
{
"age(years)":
{
"@id":
"pato:0000011",
"description":
"age of a sample (organism) at the time of data acquisition in years",
"unit":
"uo:0000036",
"unit_label":
"year"
},
"bids":
{
"@id":
"http://bids.neuroimaging.io/bids_spec1.0.2.pdf#",
"description":
"ad-hoc vocabulary for the Brain Imaging Data Structure (BIDS) standard",
"type":
"http://purl.org/dc/dcam/VocabularyEncodingScheme"
}
},
"BIDSVersion":
"1.0.2",
"author":
[
"Yoichi Miyawaki",
"Hajime Uchida",
"Okito Yamashita",
"Masa-aki Sato",
"Yusuke Morito",
"Hiroki C. Tanabe",
"Norihiro Sadato",
"Yukiyasu Kamitani"
],
"citation":
[
"Miyawaki Y, Uchida H, Yamashita O, Sato M, Morito Y, Tanabe HC, Sadato Norihiro & Kamitani Y (2008) Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders. Neuron 60:915-929."
],
"conformsto":
"http://bids.neuroimaging.io/bids_spec1.0.2.pdf",
"description":
"# Visual image reconstruction\n\nOriginal paper: Miyawaki Y, Uchida H, Yamashita O, Sato M, Morito Y, Tanabe HC, Sadato N & Kamitani Y (2008) Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders. Neuron 60:915-929.\n\n## Overview\n\nThis is the fMRI data from Miyawaki et al. (2008) \"Visual image reconstruction from human brain activity using a combination of multiscale local image decoders\". Neuron 60:915-29. In this study, we collected fMRI activity from subjects viewing images, and constructed decoders predicting local image contrast at multiple spatial scales. The combined decoders based on a linear model successfully reconstructed presented stimuli from fMRI activity.\n\n## Task\n\nThe experiment consisted of human subjects viewing contrast-based images of 12 x 12 flickering patches. There were two types of image viewing tasks: (1) random image viewing and (2) figure image (geometric shape or alphabet letter) viewing. For image presentation, a block design was used with rest periods between the presentation of each image. For random image patch presentation, images were presented for 6 s, followed by 6 s rest. For figure image presentation, images were presented for 12 s, followed by 12 s rest. The data from random image viewing runs were used to train the decoding models, and the trained model were evaluated with the data from figure image viewing runs.\n\n## Dataset\n\nThis dataset contains two subjects ('sub-01' and 'sub-02'). The subjects performed two sessions of fMRI experiments ('ses-01' and 'ses-02'). Each session is composed of several EPI runs (TR, 2000 ms; TE, 30 ms; flip angle, 80°; voxel size, 3 × 3 × 3 mm; FOV, 192 × 192 mm; number of slices, 30, slice gap, 0 mm) and inplane T2-weighted imaging (TR, 6000 ms; TE, 57 ms; flip angle, 90°; voxel size, 0.75 × 0.75 × 3.0 mm; FOV, 192 × 192 mm). The EPI images covered the entire occipital lobe. The dataset also includes a T1-weighted anatomical reference image for each subject (TR, 2250 ms; TE, 2.98 ms for sub-01 and 3.06 ms for sub-02; TI, 900 ms; flip angle, 9°; voxel size, 1.0 × 1.0 × 1.0 mm; FOV, 256 × 256 mm). The T1w images were obtained in sessions different from the fMRI experiment sessions and stored in 'ses-anat' directories. The T1w images were defaced by pydeface (<https://pypi.python.org/pypi/pydeface>). All DICOM files are converted to Nifti-1 files by mri_convert in FreeSurfer. In addition, the dataset contains mask images of manually defined ROIs for each subjects in `sourcedata` directory (See `README` in `sourcedata` for more details).\n\nDuring fMRI runs, the subject viewed contrast-based images of 12 × 12 flickering image patches. Two types of runs ('viewRandom' and 'viewFigure') were included in the experiment. In 'viewRandom' runs, random images were presented as visual stimuli. Each 'viewRandom' runs consisted of 22 stimulus presentation trials and lasted for 298 s (149 volumes). The two subjects performed 20 'viewRandom' runs. In 'viewFigure' runs, either geometric shape pattern (square, small frame, large frame, plus, X) or alphabet letter pattern (n, e, u, r, o) was presented in each trial. In addition, data while the subject viewed thin and large alphabet letter patterns (n, e, u, r, o) are included in the dataset (they are not included in the results of the original study). Each 'viewFigure' run consisted of 10 stimulus presentation trials and lasted for 268 s (134 volumes). The 'sub-01' and 'sub-02' performed 12 and 10 'viewFigure' runs, respectively.\n\nTo help subjects suppress eye blinks and firmly fixate the eyes, the color of the fixation spot changed from white to red 2 s before each stimulus block started. To ensure alertness, subjects were instructed to detect the color change of the fixation (red to green, 100 ms) that occurred after a random interval of 3–5 s from the beginning of each stimulus block. Performances of the subject was monitored online during experiments, but were not recorded and omitted from the dataset.\n\n### Task event files\n\nThe value of `trial_type` in the task event files (`*_events.tsv`) indicates the type of each trial (block) as below.\n\n- `rest`: Rest trial (no visual stimulus).\n- `stimulus_random`: Random pattern.\n- `stimulus_shape`: Geometric shape pattern (square, small frame, large frame, plus, X).\n- `stimulus_alphabet`: Alphabet pattern (n, e, u, r, o).\n- `stimulus_alphabet_thin`: Thin alphabet pattern (n, e, u, r, o).\n- `stimulus_alphabet_long`: Long alphabet pattern (n, e, u, r, o).\n\nNote that the results from thin and long alphabet patterns are not included in the original paper although the data were obtained in the same sessions.\n\nAdditional column `stimulus_pattern` contains the pattern of stimuli (12 × 12) presented in each stimulus trial. It is vectorized in row-major order. Each element in the vector corresponds to a patch (1.15° × 1.15°) in a stimulus pattern. `1` and `0` represnets a flickering checkerboard and a gray area, respectively. For example, stimulus pattern of\n\n    000000000000000000000000000000000000000111111000000111111000000110011000000110011000000110011000000110011000000000000000000000000000000000000000\n\nrepresents the following stimulus.\n\n    000000000000\n    000000000000\n    000000000000\n    000111111000\n    000111111000\n    000110011000\n    000110011000\n    000110011000\n    000110011000\n    000000000000\n    000000000000\n    000000000000\n\nThe column holds 'null' for rest trials.\n\n\n### Comments added by Openfmri Curators ###\n===========================================\n\nGeneral Comments\n----------------\n\n\nDefacing\n--------\nPydeface was used on all anatomical images to ensure de-identification of subjects. The code\ncan be found at https://github.com/poldracklab/pydeface\n\nQuality Control\n---------------\nMRIQC was run on the dataset. Results are located in derivatives/mriqc. Learn more about it here: https://mriqc.readthedocs.io/en/stable/\n\nWhere to discuss the dataset\n----------------------------\n1) www.openfmri.org/dataset/ds******/ See the comments section at the bottom of the dataset\npage.\n2) www.neurostars.org Please tag any discussion topics with the tags openfmri and dsXXXXXX.\n3) Send an email to submissions@openfmri.org. Please include the accession number in\nyour email.\n\nKnown Issues\n------------\n-behavioral performance data is not accompanied this dataset as submitter didn't submit.",
"license":
"PDDL",
"name":
"Visual image reconstruction"
},
"datalad_core":
{
"@id":
"87995922-0079-11e8-9777-002590f97d84",
"ispartof":
{
"@id":
"14114028-7d08-11e6-9ce4-002590f97d84",
"type":
"dataset"
},
"refcommit":
"172579854f37fd2c4601649519326d8061a3e7f2"
},
"datalad_unique_content_properties":
{
"bids":
{
"CogAtlasID":
[
"https://cognitiveatlas.org/task/id/trm_4c8a84f20dde2/"
],
"EchoTime":
[
0.00298,
0.00306,
0.03,
0.057
],
"FlipAngle":
[
80,
9,
90
],
"InversionTime":
[
0.9
],
"MagneticFieldStrength":
[
3.0
],
"Manufacturer":
[
"SIEMENS",
"Siemens"
],
"ManufacturersModelName":
[
"MAGNETOM Trio",
"TrioTim"
],
"PulseSequenceType":
[
"Gradient Echo EPI",
"MPRAGE",
"T2-weighted Turbo Spin Echo"
],
"RepetitionTime":
[
2,
2.2,
2.25,
6
],
"SliceEncodingDirection":
[
"k"
],
"SliceTiming":
[
[
1.005,
0.0,
1.0725,
0.0675,
1.1375,
0.135,
1.205,
0.2025,
1.2725,
0.2675,
1.34,
0.335,
1.405,
0.4025,
1.4725,
0.47,
1.54,
0.5375,
1.6075,
0.6025,
1.675,
0.67,
1.74,
0.7375,
1.8075,
0.805,
1.875,
0.87,
1.9425,
0.9375
]
],
"TaskName":
[
"viewFigure",
"viewRandom"
],
"modality":
[
"anat",
"func"
],
"run":
[
"1",
"10",
"11",
"12",
"13",
"14",
"15",
"16",
"17",
"2",
"3",
"4",
"5",
"6",
"7",
"8",
"9"
],
"session":
[
"01",
"02",
"anat"
],
"subject":
[
"01",
"02"
],
"task":
[
"viewFigure",
"viewRandom"
],
"type":
[
"T1w",
"V1d",
"V1v",
"V2d",
"V2v",
"V3",
"V3A",
"V4v",
"VP",
"bold",
"events",
"group",
"inplaneT2"
]
},
"datalad_core":
{
"url":
null
},
"nifti1":
{
"cal_max":
null,
"cal_min":
null,
"datatype":
[
"float32",
"int16"
],
"description":
[
"FreeSurfer May 13 2013"
],
"dim":
[
[
3,
256,
256,
192,
1,
1,
1,
1
],
[
3,
256,
256,
30,
1,
1,
1,
1
],
[
3,
64,
64,
30,
1,
1,
1,
1
],
[
4,
64,
64,
30,
134,
1,
1,
1
],
[
4,
64,
64,
30,
149,
1,
1,
1
]
],
"freq_axis":
[
null
],
"intent":
[
"none"
],
"magic":
[
"n+1"
],
"phase_axis":
[
null
],
"pixdim":
[
[
-1.0,
1.0,
1.0,
1.0,
2.25,
1.0,
1.0,
1.0
],
[
1.0,
0.75,
0.75,
3.0,
6.0,
1.0,
1.0,
1.0
],
[
1.0,
3.0,
3.0,
3.0,
0.0,
1.0,
1.0,
1.0
],
[
1.0,
3.0,
3.0,
3.0,
2.0,
1.0,
1.0,
1.0
]
],
"qform_code":
[
"scanner"
],
"sform_code":
[
"scanner"
],
"sizeof_hdr":
[
348
],
"slice_axis":
[
null
],
"slice_duration":
[
0.0
],
"slice_end":
[
0
],
"slice_order":
[
"unknown"
],
"slice_start":
[
0
],
"spatial_resolution(mm)":
[
[
0.75,
0.75,
3.0
],
[
1.0,
1.0,
1.0
],
[
3.0,
3.0,
3.0
]
],
"t_unit":
[
"second (uo:0000010)"
],
"temporal_spacing(s)":
[
2.0
],
"toffset":
[
0.0
],
"vox_offset":
[
0.0
],
"xyz_unit":
[
"millimiter (uo:0000016)"
]
}
},
"nifti1":
{
"@context":
{
"nifti1":
{
"@id":
"https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields#",
"description":
"Ad-hoc vocabulary for NIfTI1 header fields",
"type":
"http://purl.org/dc/dcam/VocabularyEncodingScheme"
},
"spatial_resolution(mm)":
{
"@id":
"idqa:0000162",
"description":
"spatial resolution in millimeter",
"unit":
"uo:0000016",
"unit_label":
"millimeter"
},
"temporal_spacing(s)":
{
"@id":
"idqa:0000213",
"description":
"temporal sample distance in 4D (in seconds)",
"unit":
"uo:0000010",
"unit_label":
"second"
}
}
},
"xmp":
{
"@context":
{}
}
}