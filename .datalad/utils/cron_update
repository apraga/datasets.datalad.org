#!/bin/bash

export PS1='$> '
#export PS4="> "
# Let's also log the time etc
export PS4='\n> # $(date +%T.%N)\n> '


set -eu

# A rudimentary update script which would take care about auto-updating on cron
# some selected set of the datasets.  This would help to outline use cases we
# would like to have addressed.

# Use our "centralized" environment.
source /home/yoh/proj/datalad/datalad-crawl-env/venvs/dev3/bin/activate

# override possibly present local configuration for parallel
# datalad operation if python is too old
python --version | awk '{print $2;}' | grep -q '^3\.[891]' || export DATALAD_RUNTIME_MAX__JOBS=1

# Check externals
hash datalad
hash sem
hash chronic


cd "/mnt/btrfs/datasets/datalad/crawl"

# TODO: might want to just end up using singularity environment which to
# include here!
njobs=12


run1() {
    #echo "Running $@ under $PWD"
    # no chronic for now due to https://github.com/datalad/datalad/issues/5465
    #chronic "$@"
    "$@"
}
export -f run1

runj() {
    #echo "Submitting $@ under $PWD"
    #sem -j$njobs "sleep 2; echo Done running $@"
    sem -j$njobs "$@"
    #echo "Done submitting $2"
}

doall() {
    func="$1"
    shift
    # TODO: parallelize this part
    for d in "$@"; do
       runj "$func" "$d";
    done
    sem --wait
    # And then save each at a time but without parallel since we would be
    # conflicting
    #for d in "$@"; do
    if [ "$#" -lt 5 ]; then
        msg="$@"
    else
        msg="$# paths"
    fi
    run1 datalad save -d . -m "Updates from looking at $msg" "$@"
    #done
}
export -f doall

datalad_update_ff_r() (
    msg="Fast forward $1 recursively"
    #-x echo "$msg"
    run1 datalad update -d "$1" --recursive --follow parentds --merge ff-only
    datalad save -d '.' -m "$msg" "$1"
    if [ "$#" -ge 2 ]; then
        case "$2" in
        --annex-get-auto)
            # due to https://github.com/datalad/datalad/issues/4010
            run1 git -C "$1" annex get --auto;;
        *)
            echo "Unknown option $2" >&2;
            exit 1;
        esac
    fi
)
export -f datalad_update_ff_r

datalad_crawl() (
    #-x echo "Crawl $1"
    cd "$1"
    run1 datalad crawl
)
export -f datalad_crawl

datalad_crawl_doall_update() (
    msg="Recrawl and update $1"
    #-x echo "$msg"
    set +x  # could be too many
    (
    cd "$1"
    run1 datalad crawl
    # Here using git module name which could differ from path
    subds=( $(datalad -f '{gitmodule_name}' subdatasets -r -R 1) )
    doall datalad_update_ff_r ${subds[*]}
    )
    set -x
    datalad save -d '.' -m "$msg" "$1"
)
export -f datalad_crawl_doall_update

set -x

#
# Straightforward updates from other git repos
#

if [ $# -ge 1 ]; then
   dss=( $* )
 else
   # ad-hoc way to just go through all "registered"
   dss=( $( seq 1 10 ) )
 fi

for ds in ${dss[@]}; do
   case "$ds" in
     1| psychoinformatics-de/hirni-toolbox)
       datalad_update_ff_r psychoinformatics-de/hirni-toolbox --annex-get-auto;;

     2| conp-dataset) #doall
       datalad_update_ff_r \
           conp-dataset
           # might want only tags  templateflow \
       ;;
     3| openneuro)
       # Here we crawl top but then update children in parallel
       datalad_crawl_doall_update \
           openneuro;;

     4| dandisets)
       # Here we crawl top but then update children in parallel
       datalad_update_ff_r dandi/dandisets;;

     5) break;;
     *) echo "unknown dataset $ds"; exit 1;;
  esac
done

datalad diff -d . --from datalad-public/master --to HEAD -r 

# Publish since we got so far!
datalad publish --since= --to=datalad-public --missing=inherit -r

echo "done"

datalad wtf -S datalad -S dependencies -S extensions

